/// @file  classify.cpp
/// @brief classify, using OpenVINO InferenceEngine

#include <iostream>
using std::cerr;
using std::endl;
#include <stdexcept>

#include <fstream>
#include <string>
#include <sstream>
#include <vector>

#include <gflags/gflags.h>
#include <inference_engine.hpp>



///
/// MNIST 数字クラス分類
///
///
class MNISTClassification {

public:

  // コンストラクタ
  MNISTClassification(InferenceEngine::Core& ie, std::string modeldir, std::string device):
    ie(ie), insize(28 * 28), outsize(10)
  {
    // --------------------------- 1. Load inference engine instance -------------------------------------
    // Core ie;
    // ie = new InferenceEngine::Core();

    // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
    auto network = ie.ReadNetwork(modeldir + "/classification.xml");

    std::cerr << "model: " << modeldir + "/keypoints.xml" << std::endl;

    // --------------------------- 3. Configure input & output ---------------------------------------------
    // --------------------------- input  ----------------------------------------------------
    inputInfo = network.getInputsInfo().begin()->second;
    inputName = network.getInputsInfo().begin()->first;

    // inputInfo->setLayout(Layout::NHWC);   // (batchsize, BGR, height, width)
    inputInfo->setLayout(InferenceEngine::Layout::NC);  // (batchsize, length)
    inputInfo->setPrecision(InferenceEngine::Precision::FP32);

    // --------------------------- output  ----------------------------------------------------
    outputInfo = network.getOutputsInfo().begin()->second;
    outputName = network.getOutputsInfo().begin()->first;

    outputInfo->setPrecision(InferenceEngine::Precision::FP32);

    // --------------------------- 4. Loading model to the device ------------------------------------------
    executableNetwork = ie.LoadNetwork(network, device);

    // --------------------------- 5. Create infer request -------------------------------------------------
    infer_request = executableNetwork.CreateInferRequest();


    cerr << inputName << " : " << outputName << endl;
    
    result = new float[outsize];
  }

  // デストラクタ
  virtual ~MNISTClassification() {
    delete[] result;
  }

  // 推論：同期実行
  void Inference_sync(float mnistdata[])
  {
    // --------------------------- 6. Prepare input --------------------------------------------------------
    float* ibuffer = infer_request.GetBlob(inputName)->buffer();

    std::memcpy(ibuffer, mnistdata, sizeof(float) * insize);

    // --------------------------- 7. Do inference --------------------------------------------------------
    /* Running the request synchronously */
    infer_request.Infer();

    // --------------------------- 8. Process output ------------------------------------------------------
    float* obuffer = infer_request.GetBlob(outputName)->buffer();

    std::memcpy(result, obuffer, sizeof(float) * outsize);
  }

  void print_row()
  {
    // output raw
    for (int i = 0; i < outsize; i++) {
      cerr << result[i] << ", ";
    }
    cerr << endl;
  }

  
  //
  int result_class()
  {
    float max = -1000;
    int index = 0;
    for (int i = 0; i < 10; i++) {
      if (max < result[i]) {
        max = result[i];
        index = i;
      }
    }
    return index;
  }
  
  const int insize;
  const int outsize;
  float* result;
  
private:
  InferenceEngine::Core& ie;
  std::string inputName;
  InferenceEngine::InputInfo::Ptr inputInfo;
  InferenceEngine::DataPtr outputInfo;
  std::string outputName;
  InferenceEngine::ExecutableNetwork executableNetwork;
  InferenceEngine::InferRequest infer_request;
};



std::vector<std::string> split(std::string& input, char delimiter)
{
  std::istringstream stream(input);
  std::string field;
  std::vector<std::string> result;
  
  while (getline(stream, field, delimiter)) {
    result.push_back(field);
  }
  return result;
}



DEFINE_string(d, "CPU", "device: CPU or MYRIAD");
DEFINE_string(m, "", "directory of model.xml");
DEFINE_string(i, "", "csvfile name");

int main(int argc, char *argv[])
{
  try {

    gflags::ParseCommandLineFlags(&argc, &argv, true);

    InferenceEngine::Core ie;   // 推論コア

    MNISTClassification mc(ie, FLAGS_m, FLAGS_d);

    //
    if (FLAGS_m.empty() || FLAGS_i.empty()) {
    
    cerr << "Usage: classify -i csvfile --m xmlfile" << endl;
    
      return EXIT_SUCCESS;
    }

    //
    // csv ファイル
    //

    float mnistdata[28 * 28];
    
    std::ifstream ifs(FLAGS_i);
    std::string line;
    int n = 0;

    int total = 0;
    int correct = 0;
    
    while(getline(ifs, line)) {

      std::vector<std::string> strvec = split(line, ',');

      int gt = stoi(strvec.at(0));
      
      for (int i = 1; i < strvec.size(); i++){
        mnistdata[i - 1] = stof(strvec.at(i)) / 255.0;
      }

      mc.Inference_sync(mnistdata);

      // mc.print_row();

      int pred = mc.result_class();

      if (gt == pred) {
        correct++;
      }

      total++;
    }

    cerr << "count: " << correct << " / " << total << endl;
    cerr << "accuracy: " << (float)correct / (float)total << endl;

    
    return EXIT_SUCCESS;
  }

  // -*- error -*- //

  catch (std::bad_alloc &e) {
    std::cerr << "BAD ALLOC Exception : " << e.what() << std::endl;
    return EXIT_FAILURE;
  }

  catch (const std::exception& e) {
    std::cerr << "Error: "  << e.what() << std::endl;
    return EXIT_FAILURE;
  }

  catch (...) {
    std::cerr << "unknown exception" << std::endl;
    return EXIT_FAILURE;
  }
}

/// Local Variables: ///
/// truncate-lines:t ///
/// End: ///
